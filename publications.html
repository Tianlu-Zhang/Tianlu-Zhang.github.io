
<!DOCTYPE html>
<html lang="en">
<head><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-3974203-1', 'auto'); ga('send', 'pageview');</script>
    <meta charset="UTF-8">
    <title>Heng Fan - Selected Publications</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="./stylefiles/global.css">
    <link rel="stylesheet" type="text/css" href="./stylefiles/navigation.css">
	<link rel="stylesheet" type="text/css" href="./stylefiles/home.css">
	<link rel="shortcut icon" href="./personal/favicon.ico" />
	<style>a{ TEXT-DECORATION:none}a:hover{TEXT-DECORATION:underline }</style>
</head>
<body>

<div class="navi central_body">
    <a class="navi" href="./index.html">Home</a>
    <a class="navi navi_active" href="./publications.html">Selected Publications</a>
	<a class="navi" href="./group.html">Group</a>
    <a class="navi" href="./teaching.html">Teaching</a>
    <a class="navi" href="./service.html">Professional Activities</a>
	<!--<a class="navi" href="./student.html">Student Supervision</a>-->
</div>

<div class="navi_bar"></div>

<div class="central_body">
	<!-- full journal papers -->

	<font size="3">For full publication list, please go to my <a href="https://scholar.google.com/citations?user=MVQYJiMAAAAJ" target="https://scholar.google.com/citations?user=MVQYJiMAAAAJ"><font color="#2D61FF">Google Scholar profile</font></a>.</font>
	<br>
	<!-- Note: * equal contribution &nbsp; &dagger;equal advising -->
	<!-- <HR> -->
	
	<!-- <font size="4" color="#2D61FF"><b>Pre-prints</b></font>
	<br> -->

	<!-- <table border=0 cellpadding=4> -->
		
		<!-- 
		
		<tr>
			<td><img src="./publication/vasttrack.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">VastTrack: Vast Category Visual Object Tracking</font>
					<br>
					<font color="#000000" >L. Peng, J. Gao, X. Liu, W. Li, S. Dong, Z. Zhang, H. Fan&dagger;, and L. Zhang&dagger; (&dagger;equal advising)</font>
					<br>
					<font>	arXiv:2403.03493, 2024. </font>
					<br>
					<a href="https://arxiv.org/abs/2403.03493" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://github.com/HengLan/VastTrack" target="_blank"><font color="#2D61FF">code-benchmark</font></a>  
					<br>
				</p>
			</td>
		</tr>
		
		<tr>
			<td><img src="./publication/fgd.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Flow-Guided Diffusion for Video Inpainting</font>
					<br>
					<font color="#000000" >B. Gu, Y. Yu, H. Fan, and L. Zhang</font>
					<br>
					<font>arXiv:2311.15368, 2023. </font>
					<br>
					<a href="https://arxiv.org/abs/2311.15368" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://github.com/NevSNev/FGDVI" target="_blank"><font color="#2D61FF">code</font></a>  
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/dmt.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Deficiency-Aware Masked Transformer for Video Inpainting</font>
					<br>
					<font color="#000000" >Y. Yu, H. Fan, and L. Zhang</font>
					<br>
					<font>arXiv:2307.08629, 2023. </font>
					<br>
					<a href="https://arxiv.org/abs/2307.08629" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://github.com/yeates/DMT" target="_blank"><font color="#2D61FF">code</font></a>  
					<br>
				</p>
			</td>
		</tr> -->

		 <!-- <tr>
			<td><img src="./publication/semo.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Augment and Criticize: Exploring Informative Samples for Semi-Supervised Monocular 3D Object Detection</font>
					<br>
					<font color="#000000" >Z. Li, Z. Zhang, H. Fan, Y. He, K. Wang, X. Liu, and J. Jiang</font>
					<br>
					<font>arXiv:2303.11243, 2023. </font>
					<br>
					<a href="https://arxiv.org/abs/2303.11243" target="_blank"><font color="#2D61FF">paper</font></a>
					<br>
				</p>
			</td>
		</tr> -->

	<!-- </table> -->

	<HR>
		<font size="4" color="black"><b>2024 / in press</b></font>
		<br>
	
		<table border=0 cellpadding=4>
			
			<tr>
				<td><img src="./publication/cyclicrefiner.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
				<td valign=top>
					<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
						<font style="font-weight:bold">Cyclic Refiner: Object-Aware Temporal Representation Learning for Multi-View 3D Detection and Tracking</font>
						<br>
						<font color="#000000" >M. Guo, Z. Zhang, L. Jing, Y. He, K. Wang, and H. Fan</font>
						<br>
						<font>International Journal of Computer Vision (<b>IJCV</b>), 2024, accepted.</font>
						<br>
						<a href="./publication/CyclicRefiner.pdf" target="_blank"><font color="#2D61FF">paper</font></a> 
						<br>
					</p>
				</td>
			</tr>

			<tr>
				<td><img src="./publication/smotsmall.gif", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
				<td valign=top>
					<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
						<font style="font-weight:bold">Beyond MOT: Semantic Multi-Object Tracking</font>
						<br>
						<font color="#000000" >Y. Li, Q. Li, H. Wang, X. Ma, J. Yao, S. Dong, H. Fan&dagger;, and L. Zhang&dagger; (&dagger;equal advising)</font>
						<br>
						<font>European Conference on Computer Vision (<b>ECCV</b>), 2024.</font>
						<br>
						<a href="https://arxiv.org/abs/2403.05021" target="_blank"><font color="#2D61FF">paper</font></a>  &nbsp; <a href="https://github.com/HengLan/SMOT" target="_blank"><font color="#2D61FF">code-data</font></a> 
						<br>
					</p>
				</td>
			</tr>
	
			<tr>
				<td><img src="./publication/lorat.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
				<td valign=top>
					<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
						<font style="font-weight:bold">Tracking Meets LoRA: Faster Training, Larger Model, Stronger Performance</font>
						<br>
						<font color="#000000" >L. Lin, H. Fan, Z. Zhang, Y. Wang, Y. Xu, and H. Ling</font>
						<br>
						<font>European Conference on Computer Vision (<b>ECCV</b>), 2024.</font>
						<br>
						<a href="https://arxiv.org/abs/2403.05231" target="_blank"><font color="#2D61FF">paper</font></a>  &nbsp; <a href="https://github.com/LitingLin/LoRAT" target="_blank"><font color="#2D61FF">code</font></a> 
						<br>
					</p>
				</td>
			</tr>
			
			<tr>
				<td><img src="./publication/dplnet.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
				<td valign=top>
					<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
						<font style="font-weight:bold">Efficient Multimodal Semantic Segmentation via Dual-Prompt Learning</font>
						<br>
						<font color="#000000" >S. Dong, Y. Feng, Q. Yang, Y. Huang, D. Liu, and H. Fan</font>
						<br>
						<font>IEEE/RSJ International Conference on Intelligent Robots and Systems (<b>IROS</b>), 2024. (<b>Oral</b>)</font>
						<br>
						<a href="https://arxiv.org/abs/2312.00360" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://github.com/ShaohuaDong2021/DPLNet" target="_blank"><font color="#2D61FF">code</font></a>  
						<br>
					</p>
				</td>
			</tr>

			<tr>
				<td><img src="./publication/sicp.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
				<td valign=top>
					<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
						<font style="font-weight:bold">SiCP: Simultaneous Individual and Cooperative Perception for 3D Object Detection in Connected and Automated Vehicles</font>
						<br>
						<font color="#000000" >D. Qu, Q. Chen, T. Bai, A. Qin, H. Lu, H. Fan, S. Fu, and Q. Yang</font>
						<br>
						<font>IEEE/RSJ International Conference on Intelligent Robots and Systems (<b>IROS</b>), 2024. (<b>Oral</b>)</font>
						<br>
						<a href="https://arxiv.org/abs/2312.04822" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://github.com/DarrenQu/SiCP" target="_blank"><font color="#2D61FF">code</font></a>  
						<br>
					</p>
				</td>
			</tr>

			<td><img src="./publication/mga.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
				<td valign=top>
					<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
						<font style="font-weight:bold">Robust Domain Adaptive Object Detection with Unified Multi-Granularity Alignment</font>
						<br>
						<font color="#000000" >L. Zhang, W. Zhou, H. Fan&dagger;, T. Luo, and H. Ling (&dagger;corresponding author)</font>
						<br>
						<font>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>PAMI</b>), 2024, accepted. </font>
						<br>
						<a href="https://arxiv.org/abs/2301.00371" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://github.com/tiankongzhang/MGA" target="_blank"><font color="#2D61FF">code</font></a>
						<br>
					</p>
				</td>

			<tr>
				<td><img src="./publication/vlt23.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
				<td valign=top>
					<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
						<font style="font-weight:bold">Divert More Attention to Vision-Language Object Tracking</font>
						<br>
						<font color="#000000" >M. Guo, Z. Zhang, L. Jing, H. Ling, and H. Fan</font>
						<br>
						<font>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>PAMI</b>), 2024, accepted. </font>
						<br>
						<a href="https://arxiv.org/abs/2307.10046" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://github.com/JudasDie/SOTS" target="_blank"><font color="#2D61FF">code</font></a>
						<br>
					</p>
				</td>
			</tr>

			<tr>
				<td><img src="./publication/attmot.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
				<td valign=top>
					<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
						<font style="font-weight:bold">AttMOT: Improving Multiple-Object Tracking by Introducing Auxiliary Pedestrian Attributes</font>
						<br>
						<font color="#000000" >Y. Li, Z. Xiao, L. Yang, D. Meng, X. Zhou, H. Fan, and L. Zhang</font>
						<br>
						<font>IEEE Transactions on Neural Networks and Learning Systems (<b>T-NNLS</b>), 2024, accepted. </font>
						<br>
						<a href="https://arxiv.org/abs/2308.07537" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://github.com/HengLan/AttMOT" target="_blank"><font color="#2D61FF">code</font></a>
						<br>
					</p>
				</td>
			</tr>

			<tr>
				<td><img src="./publication/cgstvg.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
				<td valign=top>
					<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
						<font style="font-weight:bold">Context-Guided Spatio-Temporal Video Grounding</font>
						<br>
						<font color="#000000">X. Gu*, H. Fan*, Y. Huang, T. Luo, and L. Zhang (*equal contribution)</font>
						<br>
						<font>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024 </font>
						<br>
						<a href="https://arxiv.org/abs/2401.01578" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="./publication/CG-STVG-CVPR24-poster.pdf" target="_blank"><font color="#2D61FF">poster</font></a> &nbsp; <a href="https://github.com/HengLan/CGSTVG" target="_blank"><font color="#2D61FF">code</font></a>  
						<br>
					</p>
				</td>
			</tr>

			<tr>
				<td><img src="./publication/motionlearner2.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
				<td valign=top>
					<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
						<font style="font-weight:bold">ProMotion: Prototypes As Motion Learners</font>
						<br>
						<font color="#000000">Y. Lu, D. Liu, Q. Wang, C. Han, Y. Cui, Z. Cao, X. Zhang, Y. Chen, and H. Fan</font>
						<br>
						<font>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024 </font>
						<br>
						<a href="https://arxiv.org/abs/2406.04999" target="_blank"><font color="#2D61FF">paper</font></a> 
						<br>
					</p>
				</td>
			</tr>

			<tr>
				<td><img src="./publication/textdet.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
				<td valign=top>
					<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
						<font style="font-weight:bold">Kernel Adaptive Convolution for Scene Text Detection via Distance Map Prediction</font>
						<br>
						<font color="#000000">J. Zheng, H. Fan, and L. Zhang</font>
						<br>
						<font>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024 </font>
						<br>
						<a href="./publication/Text_detection_CVPR_2024_paper.pdf" target="_blank"><font color="#2D61FF">paper</font></a> 
						<br>
					</p>
				</td>
			</tr>

	
			<tr>
				<td><img src="./publication/magic.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
				<td valign=top>
					<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
						<font style="font-weight:bold">MaGIC: Multi-modality Guided Image Completion</font>
						<br>
						<font color="#000000">H. Wang*, Y. Yu*, T. Luo, H. Fan, and L. Zhang (*equal contribution)</font>
						<br>
						<font>International Conference on Learning Representations (<b>ICLR</b>), 2024. </font>
						<br>
						<a href="https://arxiv.org/abs/2305.11818" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="http://www.yongshengyu.com/MaGIC-Page/" target="_blank"><font color="#2D61FF">project</font></a> &nbsp; <a href="https://github.com/yeates/MaGIC" target="_blank"><font color="#2D61FF">code</font></a> 
						<br>
					</p>
				</td>
			</tr>

			<tr>
				<td><img src="./publication/gebd.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
				<td valign=top>
					<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
						<font style="font-weight:bold">Local Compressed Video Stream Learning for Generic Event Boundary Detection</font>
						<br>
						<font color="#000000" >L. Zhang, X. Gu, C. Li, T. Luo, and H. Fan</font>
						<br>
						<font>International Journal of Computer Vision (<b>IJCV</b>), 132: 1187-1204, 2024. </font>
						<br>
						<a href="https://arxiv.org/abs/2309.15431" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://github.com/GX77/LCVSL" target="_blank"><font color="#2D61FF">code</font></a>
						<br>
					</p>
				</td>
			</tr>
	
			<tr>
				<td><img src="./publication/sspnet.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
				<td valign=top>
					<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
						<font style="font-weight:bold">SSPNet: Scale and Spatial Priors Guided Generalizable and Interpretable Pedestrian Attribute Recognition</font>
						<br>
						<font color="#000000">J. Shen, T. Guo, X. Zuo, H. Fan, and W. Yang</font>
						<br>
						<font>Pattern Recognition (<b>PR</b>), 148: 110194, 2024. </font>
						<br>
						<a href="https://arxiv.org/abs/2312.06049" target="_blank"><font color="#2D61FF">paper</font></a>
						<br>
					</p>
				</td>
			</tr>

			<tr>
				<td><img src="./publication/ica.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
				<td valign=top>
					<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
						<font style="font-weight:bold">ICAFusion: Iterative Cross-Attention Guided Feature Fusion for Multispectral Object Detection</font>
						<br>
						<font color="#000000">J. Shen, Y. Chen, Y. Liu, X. Zuo, H. Fan, and W. Yang</font>
						<br>
						<font>Pattern Recognition (<b>PR</b>), 145: 109913, 2024. </font>
						<br>
						<a href="https://arxiv.org/abs/2308.07504" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://github.com/chanchanchan97/ICAFusion" target="_blank"><font color="#2D61FF">code</font></a>
						<br>
					</p>
				</td>
			</tr>
			
	
		</table>


	<HR>
	<font size="4" color="black"><b>2023</b></font>
	<br>

	<table border=0 cellpadding=4>

		<tr>
			<td><img src="./publication/sigspatial-23.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">A Multi-granularity Decade-Long Geo-Tagged Twitter Dataset for Spatial Computing</font>
					<br>
					<font color="#000000" >Y. Feng, Z. Meng, C. Clemmer, H. Fan, and Y. Huang</font>
					<br>
					<font>ACM International Conference on Advances in Geographic Information Systems (<b>SIGSPATIAL</b>), 2023 </font>
					<br>
					<a href="./publication/SIGSPATIAL-2023.pdf" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://sigspatial.yunhefeng.me/" target="_blank"><font color="#2D61FF">project-data</font></a>
					<br>
				</p>
			</td>
		</tr>
		
		<tr>
			<td><img src="./publication/pid.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">PIDray: A Large-scale X-ray Benchmark for Real-World Prohibited Item Detection</font>
					<br>
					<font color="#000000" >L. Zhang, L. Jiang, R. Ji, and H. Fan</font>
					<br>
					<font>International Journal of Computer Vision (<b>IJCV</b>), 131: 3170-3192, 2023. </font>
					<br>
					<a href="https://arxiv.org/abs/2211.10763" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://github.com/lutao2021/PIDray" target="_blank"><font color="#2D61FF">code-data</font></a>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/cviu-23.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Collaborative Three-Stream Transformers for Video Captioning</font>
					<br>
					<font color="#000000" >H. Wang, L. Zhang, H. Fan, and T. Luo</font>
					<br>
					<font>Computer Vision and Image Understanding (<b>CVIU</b>), 235: 103799, 2023.</font>
					<br>
					<a href="./publication/CVIU-COST-23.pdf" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://github.com/wanghao14/COST" target="_blank"><font color="#2D61FF">code</font></a>
					<br>
				</p>
			
			</td>
		</tr>
		
		<tr>
			<td><img src="./publication/nsa.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Unsupervised Domain Adaptive Detection with Network Stability Analysis</font>
					<br>
					<font color="#000000" >W. Zhou*, H. Fan*, T. Luo, and L. Zhang (*equal contribution)</font>
					<br>
					<font>IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), 2023.</font>
					<br>
					<a href="https://arxiv.org/pdf/2308.08182.pdf" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://github.com/tiankongzhang/NSA" target="_blank"><font color="#2D61FF">code</font></a>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/unist.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold"> Two Birds, One Stone: A Unified Framework for Joint Learning of Image and Video Style Transfers</font>
					<br>
					<font color="#000000" >B. Gu, H. Fan, and L. Zhang</font>
					<br>
					<font>IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), 2023.</font>
					<br>
					<a href="https://arxiv.org/abs/2304.11335" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://github.com/NevSNev/UniST" target="_blank"><font color="#2D61FF">code</font></a>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/captioning-iccv-23.png", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Accurate and Fast Compressed Video Captioning</font>
					<br>
					<font color="#000000" >Y. Shen, X. Gu, K. Xu, H. Fan, L. Wen, and L. Zhang</font>
					<br>
					<font>IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), 2023.</font>
					<br>
					<a href="https://arxiv.org/abs/2309.12867" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://github.com/acherstyx/CoCap" target="_blank"><font color="#2D61FF">code</font></a>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/planartrack.gif", style="border-radius:5% 5% 5% 5%; border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">PlanarTrack: A Large-scale Challenging Benchmark for Planar Object Tracking</font>
					<br>
					<font color="#000000" >X. Liu*, X. Liu*, Z. Yi*, X. Zhou*, T. Le, L. Zhang, Y. Huang, Q. Yang, and H. Fan (*equal contribution)</font>
					<br>
					<font>IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), 2023.</font>
					<br>
					<a href="https://arxiv.org/abs/2303.07625" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://hengfan2010.github.io/projects/PlanarTrack/" target="_blank"><font color="#2D61FF">code-data</font></a>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/animaltrack.gif", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">AnimalTrack: A Benchmark for Multi-Animal Tracking in the Wild</font>
					<br>
					<font color="#000000" >L. Zhang*, J. Gao*, Z. Xiao, and H. Fan (*equal contribution)</font>
					<br>
					<font>International Journal of Computer Vision (<b>IJCV</b>), 131: 496-513, 2023.</font>
					<br>
					<a href="https://arxiv.org/abs/2205.00158" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://hengfan2010.github.io/projects/AnimalTrack/" target="_blank"><font color="#2D61FF">project with data</font></a>
					<br>
				</p>
			
			</td>
		</tr>

		<!-- 
		<tr>
			<td><img src="./publication/vot.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Visual Object Tracking: Progress, Challenge, and Future</font>
					<br>
					<font color="#000000">L. Zhang and H. Fan</font>
					<br>
					<font>The Innovation, 4(2), 100402, 2023. (Invited)</font>
					<br>
					<a href="https://www.cell.com/the-innovation/fulltext/S2666-6758(23)00030-9" target="_blank"><font color="#2D61FF">paper</font></a>
					<br>
				</p>
			</td>
		</tr>
		-->

	</table>



	<HR>
	<font size="4" color="black"><b>2022</b></font>
	<br>

	<table border=0 cellpadding=4>
			

		<tr>
			<td><img src="./publication/swintrack.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">SwinTrack: A Simple and Strong Baseline for Transformer Tracking</font>
					<br>
					<font color="#000000" >L. Lin*, H. Fan*, Z. Zhang, Y. Xu, and H. Ling (*equal contribution)</font>
					<br>
					<font>	Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2022.</font>
					<br>
					<a href="./publication/SwinTrack-NeurIPS-2022.pdf" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="./publication/swintrack-poster.pdf" target="_blank"><font color="#2D61FF">poster</font></a> &nbsp; <a href="https://github.com/HengLan/SwinTrack" target="_blank"><font color="#2D61FF">code</font></a>
					<br>
				</p>
			
			</td>
		</tr>

		<tr>
			<td><img src="./publication/vlt.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Divert More Attention to Vision-Language Tracking</font>
					<br>
					<font color="#000000" >M. Guo*, Z. Zhang*, H. Fan, and L. Jing (*equal contribution)</font>
					<br>
					<font>	Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2022.</font>
					<br>
					<a href="./publication/VLT-NeurIPS-2022.pdf" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="./publication/vlt-poster.pdf" target="_blank"><font color="#2D61FF">poster</font></a> &nbsp; <a href="https://github.com/JudasDie/SOTS" target="_blank"><font color="#2D61FF">code</font></a>
					<br>
				</p>
			
			</td>
		</tr>

		<tr>
			<td><img src="./publication/invertfill.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">High-Fidelity Image Inpainting with GAN Inversion</font>
					<br>
					<font color="#000000" >Y. Yu, L. Zhang, H. Fan, and T. Luo</font>
					<br>
					<font>European Conference on Computer Vision (<b>ECCV</b>), 2022.</font>
					<br>
					<a href="./publication/InvertFill-ECCV-2022.pdf" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="./publication/InvertFill-ECCV-2022-Supp.pdf" target="_blank"><font color="#2D61FF">supplementary</font></a> 
					<br>
				</p>
			
			</td>
		</tr>

		<tr>
			<td><img src="./publication/media.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Towards Bridging the Distribution Gap: Instance to Prototype Earth Mover’s Distance for Distribution Alignment</font>
					<br>
					<font color="#000000" >Q. Zhou, R. Wang, G. Zeng, H. Fan, and G. Zheng</font>
					<br>
					<font>Medical Image Analysis (<b>MedIA</b>), 82: 102607, 2022.</font>
					<br>
					<a href="./publication/MedIA-2022.pdf" target="_blank"><font color="#2D61FF">paper</font></a>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/visdrone.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Detection and Tracking Meet Drones Challenge</font>
					<br>
					<font color="#000000" >P. Zhu, L. Wen, D. Du, X. Bian, H. Fan, Q. Hu, and H. Ling</font>
					<br>
					<font>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>PAMI</b>), 44(11): 7380-7399, 2022.</font>
					<br>
					<a href="https://arxiv.org/abs/2001.06303" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://github.com/VisDrone/VisDrone-Dataset" target="_blank"><font color="#2D61FF">code and project</font></a>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/glgan.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">GL-GAN: Adaptive Global and Local Bilevel Optimization for Generative Adversarial Network</font>
					<br>
					<font color="#000000" >Y. Liu, H. Fan, X. Yuan, and J. Xiang</font>
					<br>
					<font>Pattern Recognition (<b>PR</b>), 123: 108375, 2022.</font>
					<br>
					<a href="publication\PR-22.pdf" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://github.com/summar6/GL-GAN" target="_blank"><font color="#2D61FF">code</font></a>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/inbn.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Learning Target-aware Representation for Visual Tracking via Informative Interactions</font>
					<br>
					<font color="#000000" >M. Guo, Z. Zhang, H. Fan, L. Jing, Y. Lyu, B. Li, and W. Hu</font>
					<br>
					<font>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>), 2022. (<b>Long Oral</b>)</font>
					<br>
					<a href="https://arxiv.org/abs/2201.02526" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://github.com/JudasDie/SOTS" target="_blank"><font color="#2D61FF">code</font></a>
					<br>
				</p>
			
			</td>
		</tr>

	</table>

	
	<HR>
	<font size="4" color="black"><b>2021</b></font>
	<br>

	<table border=0 cellpadding=4>

		<tr>
			<td><img src="./publication/totb.gif", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Transparent Object Tracking Benchmark</font>
					<br>
					<font color="#000000">H. Fan, H. Miththanthaya, Harshit, S. Rajan, X. Liu, Z. Zou, Y. Lin, and H. Ling</font>
					<br>
					<font>IEEE International Conference on Computer Vision (<b>ICCV</b>), 2021.</font>
					<br>
					<a href="https://arxiv.org/abs/2011.10875" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://hengfan2010.github.io/projects/TOTB/" target="_blank"><font color="#2D61FF">code and project</font></a>
					<br>
				</p>
			
			</td>
		</tr>

		<tr>
			<td><img src="./publication/cract.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">CRACT: Cascaded Regression-Align-Classification for Robust Visual Tracking</font>
					<br>
					<font color="#000000">H. Fan and H. Ling</font>
					<br>
					<font>IEEE/RSJ International Conference on Intelligent Robots and Systems (<b>IROS</b>), 2021.</font>
					<br>
					<a href="https://arxiv.org/abs/2011.12483" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="#" target="_blank"><font color="#2D61FF">project</font></a>
					<br>
				</p>
			</td>
		</tr>		

		<tr>
			<td><img src="./publication/lasotijcv.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">LaSOT: A High-quality Large-scale Single Object Tracking Benchmark</font>
					<br>
					<font color="#000000" >H. Fan, H. Bai, L. Lin, F. Yang, P. Chu, G. Deng, S. Yu, Harshit, M. Huang, J. Liu, Y. Xu, C. Liao, L. Yuan, and H. Ling</font>
					<br>
					<font>International Journal of Computer Vision (<b>IJCV</b>), 129: 439-461, 2021.</font>
					<br>
					<a href="publication\IJCV-21.pdf" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="http://vision.cs.stonybrook.edu/~lasot/" target="_blank"><font color="#2D61FF">code and benchmark</font></a>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/clsgan.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">ClsGAN: Selective Attribute Editing Based On Classification Adversarial Network</font>
					<br>
					<font color="#000000" >Y. Liu, H. Fan, F. Ni, and J. Xiang</font>
					<br>
					<font>Neural Networks (<b>NN</b>), 133: 220-228, 2021.</font>
					<br>
					<a href="publication\NN-2021.pdf" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://github.com/summar6/ClsGAN" target="_blank"><font color="#2D61FF">code</font></a>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/tracklinic.gif", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">TracKlinic: Diagnosis of Challenge Factors in Visual Tracking</font>
					<br>
					<font color="#000000">H. Fan, F. Yang, P. Chu, Y. Lin, L. Yuan, and H. Ling</font>
					<br>
					<font>IEEE Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2021.</font>
					<br>
					<a href="https://openaccess.thecvf.com/content/WACV2021/papers/Fan_TracKlinic_Diagnosis_of_Challenge_Factors_in_Visual_Tracking_WACV_2021_paper.pdf" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href=".\projects\TracKlinic\TracKlinic.htm" target="_blank"><font color="#2D61FF">project</font></a>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/mart.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">MART: Motion-Aware Recurrent Neural Network for Robust Visual Tracking</font>
					<br>
					<font color="#000000">H. Fan and H. Ling</font>
					<br>
					<font>IEEE Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2021.</font>
					<br>
					<a href="https://openaccess.thecvf.com/content/WACV2021/papers/Fan_MART_Motion-Aware_Recurrent_Neural_Network_for_Robust_Visual_Tracking_WACV_2021_paper.pdf" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="#" target="_blank"><font color="#2D61FF">project</font></a>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/regct.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Robust and Efficient Graph Correspondence Transfer for Person Re-identification</font>
					<br>
					<font color="#000000" >Q. Zhou, H. Fan, H. Yang, H. Su, S. Zheng, S. Wu, and H. Ling</font>
					<br>
					<font>IEEE Transactions on Image Processing (<b>T-IP</b>), 30: 1623-1638, 2021.</font>
					<br>
					<font><a href="publication\TIP-19-b.pdf" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://drive.google.com/file/d/1rt7DpXJIvRVCya463gt2jqxl71xy5o7J/view" target="_blank"><font color="#2D61FF">code</font></a></font>
					<br>
				</p>
			</td>
		</tr>

	</table>



	<HR>
	<font size="4" color="black"><b>2020</b></font>
	<br>

	<table border=0 cellpadding=4>

		<tr>
			<td><img src="./publication/wbc.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Weighted Bilinear Coding Over Salient Body Parts for Person Re-identification</font>
					<br>
					<font color="#000000" >Z. Chang, Q. Zhou, H. Fan, H. Yang, H. Su, S. Zheng, and H. Ling</font>
					<br>
					<font>Neurocomputing, 407: 454-464, 2020.</font>
					<br>
					<font><a href="publication\Neurocomputing-20.pdf" target="_blank"><font color="#2D61FF">paper</font></a>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/embc2020.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Detection of Trabecular Landmarks for Osteoporosis Prescreening in Dental Panoramic Radiographs</font>
					<br>
					<font color="#000000">J. Ren, H. Fan, J. Yang, and H. Ling</font>
					<br>
					<font>IEEE Engineering in Medicine and Biology Society  (<b>EMBC</b>), 2020.</font>
					<br>
					<a href="publication\EMBC-20.pdf" target="_blank"><font color="#2D61FF">paper</font></a>
					<br>
				</p>
			</td>
		</tr>

	</table>


	<HR>
	<font size="4" color="black"><b>2019</b></font>
	<br>

	<table border=0 cellpadding=4>

		<tr>
			<td><img src="./publication/clusdet.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Clustered Object Detection in Aerial Images</font>
					<br>
					<font color="#000000" >F. Yang, H. Fan, P. Chu, E. Blasch, and H. Ling</font>
					<br>
					<font>IEEE International Conference on Computer Vision (<b>ICCV</b>), 2019.</font>
					<br>
					<a href="publication\ICCV-19.pdf" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://drive.google.com/file/d/1IrrU937Vdki4ibN6lAAi7FQ-yx4CWjJw/view" target="_blank"><font color="#2D61FF">supplementary</font></a> &nbsp; <a href="https://github.com/fyangneil/Clustered-Object-Detection-in-Aerial-Image" target="_blank"><font color="#2D61FF">code</font></a>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/st.gif", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Siamese Cascaded Region Proposal Networks for Real-Time Visual Tracking</font>
					<br>
					<font color="#000000" >H. Fan and H. Ling</font>
					<br>
					<font>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2019.</font>
					<br>
					<a href="publication\CVPR-19-a.pdf" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://www3.cs.stonybrook.edu/~hling/code/CRPN/crpn.htm" target="_blank"><font color="#2D61FF">code</font></a>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/lasot.gif", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">LaSOT: A High-quality Benchmark for Large-scale Single Object Tracking</font>
					<br>
					<font color="#000000" >H. Fan, L. Lin, F. Yang, P. Chu, G. Deng, S. Yu, H. Bai, Y. Xu, C. Liao, and H. Ling</font>
					<br>
					<font>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2019.</font>
					<br>
					<a href="publication\CVPR-19-b.pdf" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="http://vision.cs.stonybrook.edu/~lasot/" target="_blank"><font color="#2D61FF">code and benchmark</font></a>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/wacv-19a-img.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Scene Parsing via Dense Recurrent Neural Networks with Attentional Selection</font>
					<br>
					<font color="#000000" >H. Fan, P. Chu, L. Latecki, and H. Ling</font>
					<br>
					<font>IEEE Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2019.</font> 
					<br>
					<a href="publication\WACV-19-a.pdf" target="_blank"><font color="#2D61FF">paper</font></a>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/wacv-19b-img.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Online Multi-Object Tracking with Instance-Aware Tracker and Dynamic Model Refreshment</font>
					<br>
					<font color="#000000" >P. Chu, H. Fan, C. Tan, and H. Ling</font>
					<br>
					<font>IEEE Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2019.</font> 
					<br>
					<a href="publication\WACV-19-b.pdf" target="_blank"><font color="#2D61FF">paper</font></a>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/ptavtip.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Parallel Tracking and Verifying</font>
					<br>
					<font color="#000000" >H. Fan and H. Ling</font>
					<br>
					<font>IEEE Transactions on Image Processing (<b>T-IP</b>), 28(8): 4130-4144, 2019.</font>
					<br>
					<font><a href="publication\TIP-19-a.pdf" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://www3.cs.stonybrook.edu/~hling/code/PTAV/ptav.htm" target="_blank"><font color="#2D61FF">code</font></a></font>
					<br>
				</p>
			</td>
		</tr>

	</table>


	<HR>
	<font size="4" color="black"><b>2018</b></font>
	<br>

	<table border=0 cellpadding=4>

		<tr>
			<td><img src="./publication/mlrnns.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Multi-level Contextual RNNs with Attention Model for Scene Labeling</font>
					<br>
					<font color="#000000" >H. Fan, X. Mei, D. Prokhorov, and H. Ling</font>
					<br>
					<font>IEEE Transactions on Intelligent Transportation Systems (<b>T-ITS</b>), 19(11): 3475-3485, 2018.</font>
					<br>
					<font><a href="publication\TITS-18.pdf" target="_blank"><font color="#2D61FF">paper</font></a></font>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/gct.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Graph Correspondence Transfer for Person Re-identification</font>
					<br>
					<font color="#000000" >Q. Zhou, H. Fan, S. Zheng, H. Su, X. Li, S. Wu, and H. Ling</font>
					<br>
					<font>AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2018.</font> 
					<br>
					<a href="publication\AAAI-18.pdf" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="https://drive.google.com/file/d/1rt7DpXJIvRVCya463gt2jqxl71xy5o7J/view" target="_blank"><font color="#2D61FF">code</font></a>
					<br>
				</p>
			</td>
		</tr>

	</table>

	<HR>
	<font size="4" color="black"><b>2017</b></font>
	<br>

	<table border=0 cellpadding=4>

		<tr>
			<td><img src="./publication/ptaviccv.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Parallel Tracking and Verifying: A Framework for Real-Time and High Accuracy Visual Tracking</font>
					<br>
					<font color="#000000" >H. Fan and H. Ling</font>
					<br>
					<font>IEEE International Conference on Computer Vision (<b>ICCV</b>), 2017.</font>
					<br>
					<a href="publication\ICCV-17.pdf" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="publication\ICCV17-poster.pdf" target="_blank"><font color="#2D61FF">poster</font></a> &nbsp; <a href="publication\ICCV17-slide.pdf" target="_blank"><font color="#2D61FF">slide</font></a> &nbsp; <a href="https://www3.cs.stonybrook.edu/~hling/code/PTAV/ptav.htm" target="_blank"><font color="#2D61FF">code</font></a>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/sanet.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">SANet: Structure-Aware Network for Visual Tracking</font>
					<br>
					<font color="#000000" >H. Fan and H. Ling</font>
					<br>
					<font>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) Workshop, 2017.</font> 
					<br>
					<a href="publication\CVPRW-17.pdf" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="http://www.dabi.temple.edu/~hbling/code/SANet/SANet.html" target="_blank"><font color="#2D61FF">code</font></a>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/lgcf.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Robust Visual Tracking via Local-Global Correlation Filter</font>
					<br>
					<font color="#000000">H. Fan and J. Xiang</font>
					<br>
					<font>AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2017.</font> 
					<br>
					<a href="publication\AAAI-17.pdf" target="_blank"><font color="#2D61FF">paper</font></a>
					<br>
				</p>
			</td>
		</tr>

		<tr>
			<td><img src="./publication/mjdl.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Robust Visual Tracking with Multitask Joint Dictionary Learning</font>
					<br>
					<font color="#000000" >H. Fan and J. Xiang</font>
					<br>
					<font>IEEE Transactions on Circuits and Systems for Video and Technology (<b>T-CSVT</b>), 27(5): 1018-1030, 2017.</font>
					<br>
					<font><a href="publication\TCSVT-17.pdf" target="_blank"><font color="#2D61FF">paper</font></a></font> &nbsp; <a href=".\projects\MJDL\MJDL.htm" target="_blank"><font color="#2D61FF">project</font></a>
					<br>
				</p>
			</td>
		</tr>

	</table>

	<HR>
	<font size="4" color="black"><b>2016</b></font>


	<table border=0 cellpadding=4>

		<tr>
			<td><img src="./publication/iv.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=120 height="80">	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Cross Datasets Vegetation Detection with Spatial Prior and Local Context</font>
					<br>
					<font color="#000000" >H. Fan, X. Mei, D. Prokhorov, and H. Ling</font>
					<br>
					<font>IEEE Intelligent Vehicles Symposium (<b>IV</b>), 2016.</font>
					<br>
					<a href="publication\IV-16.pdf" target="_blank"><font color="#2D61FF">paper</font></a> &nbsp; <a href="#"><font color="#2D61FF">code and data (by request)</font></a>
					<br>
				</p>
			</td>
		</tr>

	</table>

	<HR>
	<font size="4" color="black"><b>PhD Dissertation</b></font>
	<br>


	<table border=0 cellpadding=4>
			
		<tr>
			<td><img src="./publication/sbu.png", style="border-radius:5% 5% 5% 5%;border-color:#C8C4C3;" border="1" width=100>	</TD>
			<td valign=top>
				<p style="line-height: 110%; margin-top: 5px; margin-bottom: 5px;">
					<font style="font-weight:bold">Algorithms and Benchmarks for Robust Visual Object Tracking</font>
					<br>
					<font color="#000000" >H. Fan</font>
					<br>
					<font>Advisor: <a href="https://www3.cs.stonybrook.edu/~hling/" target="_blank"><font color="black">Prof. Haibin Ling</font></a></font>
					<br>
					<font>Committee: Professors <a href="https://www3.cs.stonybrook.edu/~gu/" target="_blank"><font color="black">Xianfeng Gu</font></a>, <a href="https://www3.cs.stonybrook.edu/~samaras/" target="_blank"><font color="black">Dimitris Samaras</font></a>, <a href="https://dentistry.temple.edu/about/faculty-staff/jie-yang-jieyang" target="_blank"><font color="black">Jie Yang</font></a></font>
					<br>
					<font>Department of Computer Science, State University of New York at Stony Brook, 2021.</font>
					<br>
					<a href="https://www.proquest.com/docview/2558097086?pq-origsite=gscholar&fromopenview=true" target="_blank"><font color="#2D61FF">pdf link</font></a>  &nbsp; <a href="https://docs.google.com/presentation/d/1GsGYDb6WazjAaIxPu2P0QzxPfu4wp4Nn/edit?usp=share_link&ouid=116478199769662584478&rtpof=true&sd=true" target="_blank"><font color="#2D61FF">slide</font></a>
					<br>
				</p>
			</td>
		</tr>
	</table>
	
	<HR>
	
	Copyright Notice: The papers presented above are to ensure timely dissemination of scholarly and technical work and only for personal or classroom use. Copyright and all rights therein are retained by authors and/or by other copyright holders.
	
	<HR>
</div>

</div>
</body>
</html>