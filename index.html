<!DOCTYPE html>
<!-- saved from url=(0042)https://cse.buffalo.edu/~jmeng2/index.html -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script async="" src="./page_files/analytics.js.download"></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-3974203-1', 'auto'); ga('send', 'pageview');</script>
    
    <title>Homepage for Heng Fan</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="./stylefiles/global.css">
    <link rel="stylesheet" type="text/css" href="./stylefiles/navigation.css">
	<link rel="stylesheet" type="text/css" href="./stylefiles/home.css">
	<link rel="shortcut icon" href="./personal/favicon.ico" />
	<style>a{ TEXT-DECORATION:none}a:hover{TEXT-DECORATION:underline }</style>
	<!-- <style>a{ TEXT-DECORATION:none }</style>-->   <!-- change style for hyperlink -->
</head>

<body data-gr-c-s-loaded="true">

<div class="navi central_body">
    <a class="navi navi_active" href="./index.html">Home</a>
    <a class="navi" href="./publications.html">Selected Publications</a>
	<a class="navi" href="./group.html">Group</a>
    <a class="navi" href="./teaching.html">Teaching</a>
    <a class="navi" href="./service.html">Professional Activities</a>
	<!-- <a class="navi" href="./student.html">Student Supervision</a> -->
</div>

<div class="navi_bar"></div>

<div class="central_body">

    <div class="photo_intro">
        <img src="./personal/hfan-2022.jpeg" alt="Heng Fan" class="photo" height="130">
        <div class="intro">
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="intro_name"><font size="5">Heng Fan</font></span>
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font size="4" style="line-height: 170%">Assistant Professor</font>
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://computerscience.engineering.unt.edu/"><font color="2D61FF">Department of Computer Science and Engineering</font></a>, <a href="https://www.unt.edu/"><font color="#2D61FF">University of North Texas</font></a>
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Office: Room F284, UNT Discovery Park, 3940 N Elm St, Denton, TX 76207
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Email: heng.fan@unt.edu<!-- hefan<img height="13" width="13"
src="personal/at.gif">cs.stonybrook.edu  -->
			<br>
			
			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[<a href="personal/Fan-selected-cv.pdf" target="_blank"><font color="#2D61FF">Curriculum Vitae</font></a>]
			[<a href="https://scholar.google.com/citations?user=MVQYJiMAAAAJ" target="https://scholar.google.com/citations?user=MVQYJiMAAAAJ"><font color="#2D61FF">Google Scholar</font></a>]
			[<a href="https://www.linkedin.com/in/%E6%81%92-%E6%A8%8A-64697aaa/" target="https://www.linkedin.com/in/%E6%81%92-%E6%A8%8A-64697aaa/"><font color="#2D61FF">LinkedIn</font></a>]
			[<a href="https://dblp.org/pid/20/10120-1.html" target="https://dblp.org/pid/20/10120-1.html"><font color="#2D61FF">DBLP</font></a>]
			[<a href="https://hengfan2010.github.io/aiseminar/" target="https://hengfan2010.github.io/aiseminar/"><font color="#2D61FF">AI Seminar</font></a>]
        </div>
    </div>
	
	<HR>
	
    <font size="4" color="black"><b>Short Bio</b></font>
    <p style="margin-top:5px;"><font>Heng Fan is currently an Assistant Professor in the <a href="https://computerscience.engineering.unt.edu/"><font color="#2D61FF">Department of Computer Science and Engineering</font></a> at the <a href="https://www.unt.edu/"><font color="#2D61FF">University of North Texas</font></a>. He received his B.S. from Huazhong Agricultural University in 2013 and Ph.D. (advised by <a href="https://www3.cs.stonybrook.edu/~hling/"><font><font color="#2D61FF">Professor Haibin Ling</font></font></a>) from Stony Brook University in 2021, respectively. His research interests include computer vision with a particular interest in various video analysis tasks using vision and language, robotics with a focus on robot vision perception, and medical image analysis. He has served as a guest editor for <i>ACM Journal on Autonomous Transportation Systems</i> (ACM JATS) and as Area Chairs for WACV 2022-2025.
    
	<HR>
	
	
	<font size="3" color="black"><b>To prospective students:</b></font> I am looking for <i><b>self-motivated</b></i> <b>PhD</b>/<b>master</b>/<b>undergraduate</b>/<b>TAMS</b> students to work on interesting problems in computer and robotic vision. If you are interested, please read <a href="./prospective-students.html" target="_blank"><font color="#2D61FF">HERE</font></a>.
	
	<!-- <font size="3" color="black"><b>To prospective students:</b></font>: <font size="3"> I am looking for <b><i>self-motivated</i> PhD students</b> to work on interesting problems in computer and robotic vision, with a focus on <i>general video analysis</i>. Solid background in mathematics and programming is required. If you're interested, please send your CV, transcript(s), and test scores to my e-mail (heng.fan@unt.edu). Publication in related fields is a plus.<br>

	&nbsp; &nbsp;&nbsp; &nbsp;In addition, motivated master and undergraduate students within UNT as well as TAMS high school students are also welcome. If you're interested in conducting research with me, we should talk.</font> -->
	
	<!-- <HR>

		<font size="3" color="red"><b>Call for papers:</b></font> <font size="3"> We're organizing a <a href="https://dl.acm.org/pb-assets/static_journal_pages/jats/pdf/JATS_SI_UAVS-CFP-1695933597020.pdf" target="_blank"><font color="#2D61FF">special issue</font></a> on <i>Advancements in Uncrewed Aerial Vehicles: Perception, Interaction, Security, Ethics, and
			Beyond</i> at ACM Journal on Autonomous Transportation Systems. Submissions on related topics are welcome. -->

	<!-- <font size="3" color="red"><b>Postdoctoral positions:</b></font> <font size="3"> Several postdoctoral openings are available. 
		The general areas of interests include deep learning, spatial intelligence, mobile computing, and robotic coordination. 
		Check out <a href="https://jobs.untsystem.edu/postings/71770" target="_blank"><font><font color="#2D61FF">HERE</font></font></a>.</font>
	
	<HR>  -->
	
	<HR>

	<font size="4" color="black"><b>News</b>&nbsp;<img style="" src="./personal/news.gif" border="0"></font>
    <ul style="margin-top:5px;">
		<li><font style="font-weight:bold; line-height: 120%">2024-07:</font> A paper on multi-view 3D object detection and tracking (<a href="https://arxiv.org/abs/2407.03240" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>) is accepted to IJCV.</li>
		<li><font style="font-weight:bold; line-height: 120%">2024-07:</font> Two papers on object tracking (<a href="https://arxiv.org/abs/2403.05021" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/HengLan/SMOT" target="_blank"><font><font color="#2D61FF">code</font></font></a>, <a href="https://arxiv.org/abs/2403.05231" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/LitingLin/LoRAT" target="_blank"><font color="#2D61FF">code</font></a>) are accepted to <a href="https://eccv2024.ecva.net/" target="_blank"><font><font color="#2D61FF">ECCV 2024</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2024-07:</font> Two <b>oral</b> papers on efficient segmentation (<a href="https://arxiv.org/abs/2312.00360" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/ShaohuaDong2021/DPLNet" target="_blank"><font><font color="#2D61FF">code</font></font></a>) and 3D detection (<a href="https://arxiv.org/abs/2312.04822" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/DarrenQu/SiCP" target="_blank"><font><font color="#2D61FF">code</font></font></a>) are accepted to <a href="https://wacv2025.thecvf.com/" target="_blank"><font><font color="#2D61FF">IROS 2024</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2024-06:</font> A paper on domain adaptive object detection (<a href="https://arxiv.org/abs/2301.00371" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/tiankongzhang/MGA" target="_blank"><font><font color="#2D61FF">code</font></font></a>) is accepted to PAMI.</li>
		<li><font style="font-weight:bold; line-height: 120%">2024-06:</font> I will serve as an Area Chair for <a href="https://wacv2025.thecvf.com/" target="_blank"><font><font color="#2D61FF">WACV 2025</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2024-05:</font> A paper on vision-language tracking (<a href="https://arxiv.org/abs/2307.10046" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/JudasDie/SOTS" target="_blank"><font><font color="#2D61FF">code</font></font></a>) is accepted to PAMI.</li>
		<li><font style="font-weight:bold; line-height: 120%">2024-03:</font> Our AttMOT (<a href="https://arxiv.org/abs/2308.07537" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/HengLan/AttMOT" target="_blank"><font><font color="#2D61FF">code</font></font></a>) is accepted to IEEE Transactions on Neural Networks and Learning Systems.</li>
		<li><font style="font-weight:bold; line-height: 120%">2024-03:</font> Present VastTrack (<a href="https://arxiv.org/abs/2403.03493" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/HengLan/VastTrack" target="_blank"><font><font color="#2D61FF">code-data</font></font></a>), containing over 2.1K classes/50K videos for vast category visual tracking.</li>
		<li><font style="font-weight:bold; line-height: 120%">2024-02:</font> Three papers (<a href="https://arxiv.org/abs/2401.01578" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/HengLan/CGSTVG" target="_blank"><font><font color="#2D61FF">code</font></font></a>, <a href="https://arxiv.org/abs/2406.04999" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>, <a href="./publication/Text_detection_CVPR_2024_paper.pdf" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>) are accepted to <a href="https://cvpr.thecvf.com/" target="_blank"><font><font color="#2D61FF">CVPR 2024</font></font></a>.</li>
		<!-- <li><font style="font-weight:bold; line-height: 120%">2024-01:</font> Welcome PhD student Bing Fan to join our team.</li>
		<li><font style="font-weight:bold; line-height: 120%">2024-01:</font> Our MaGIC for multimodal guided image completion (<a href="https://arxiv.org/abs/2305.11818" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://www.yongshengyu.com/MaGIC-Page/" target="_blank"><font><font color="#2D61FF">project</font></font></a>/<a href="https://github.com/yeates/MaGIC" target="_blank"><font><font color="#2D61FF">code</font></font></a>) is accepted to <a href="https://iclr.cc/" target="_blank"><font><font color="#2D61FF">ICLR 2024</font></font></a>.</li> -->
		<!-- 
		<li><font style="font-weight:bold; line-height: 120%">2024-01:</font> Check out our CG-STVG (<a href="https://arxiv.org/abs/2401.01578" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/HengLan/CGSTVG" target="_blank"><font><font color="#2D61FF">code</font></font></a>) on spatio-temporal video grounding.</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-12:</font> Check out our FGDVI (<a href="https://arxiv.org/abs/2311.15368" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/NevSNev/FGDVI" target="_blank"><font><font color="#2D61FF">code</font></font></a>) on flow-guided diffusion for video inpainting.</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-12:</font> Check out our DPLNet (<a href="https://arxiv.org/abs/2312.00360" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/ShaohuaDong2021/DPLNet" target="_blank"><font><font color="#2D61FF">code</font></font></a>) on efficient multimodal semantic segmentation.</li> -->
		<!-- <li><font style="font-weight:bold; line-height: 120%">2023-12:</font> A paper (<a href="https://arxiv.org/abs/2312.06049" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>) on pedestrian attribute recognition is accepted to Pattern Recognition (PR).</li> -->
		<!-- <li><font style="font-weight:bold; line-height: 120%">2023-12:</font> Check out our DPLNet (<a href="https://arxiv.org/abs/2312.00360" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/ShaohuaDong2021/DPLNet" target="_blank"><font><font color="#2D61FF">code</font></font></a>) on efficient multimodal semantic segmentation.</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-12:</font> Check out our FGDVI (<a href="https://arxiv.org/abs/2311.15368" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/NevSNev/FGDVI" target="_blank"><font><font color="#2D61FF">code</font></font></a>) for flow-guided diffusion for video inpainting.</li> -->
		<!-- <li><font style="font-weight:bold; line-height: 120%">2023-10:</font> Recognized as the World's top 2% scientists in 2022 by Standford University.</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-09:</font> A paper (<a href="https://arxiv.org/abs/2309.15431" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/GX77/LCVSL" target="_blank"><font><font color="#2D61FF">code</font></font></a>) on event boundary detection from video stream is accepted to IJCV.</li> -->

		<!--
		<li><font style="font-weight:bold; line-height: 120%">2023-09:</font> A paper (<a href="https://sigspatial.yunhefeng.me/" target="_blank"><font><font color="#2D61FF">project</font></font></a>) on spatial computing is accepted to <a href="https://sigspatial2023.sigspatial.org/" target="_blank"><font><font color="#2D61FF">SIGSPATIAL 2023</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-08:</font> A paper (<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320323006118" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/chanchanchan97/ICAFusion" target="_blank"><font><font color="#2D61FF">code</font></font></a>) on RGB-Thermal object detection is accepted to Pattern Recognition (PR).</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-08:</font> Our PIDray (<a href="https://arxiv.org/abs/2211.10763" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/lutao2021/PIDray" target="_blank"><font><font color="#2D61FF">code-data</font></font></a>) on prohibited item detection is accepted to IJCV.</li>

		
		<li><font style="font-weight:bold; line-height: 120%">2023-08:</font> Check out our DMT (<a href="https://arxiv.org/abs/2307.08629" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/yeates/DMT" target="_blank"><font><font color="#2D61FF">code</font></font></a>) for state-of-the-art video inpainting.</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-07:</font> A paper (<a href="./publication/CVIU-COST-23.pdf" target="_blank"><font color="#2D61FF">pdf</font></a>/<a href="https://github.com/wanghao14/COST" target="_blank"><font><font color="#2D61FF">code</font></font></a>) on video captioning is accepted to Computer Vision and Image Understanding (CVIU).</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-07:</font> Four papers (<a href="https://arxiv.org/pdf/2308.08182.pdf" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/tiankongzhang/NSA" target="_blank"><font><font color="#2D61FF">code</font></font></a>,  
			<a href="https://arxiv.org/abs/2304.11335" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/NevSNev/UniST" target="_blank"><font><font color="#2D61FF">code</font></font></a>, 
			<a href="https://arxiv.org/abs/2309.12867" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/acherstyx/CoCap" target="_blank"><font><font color="#2D61FF">code</font></font></a>,
			<a href="https://arxiv.org/abs/2303.07625" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://hengfan2010.github.io/projects/PlanarTrack/" target="_blank"><font><font color="#2D61FF">code-data</font></font></a>) are accepted to <a href="https://iccv2023.thecvf.com/" target="_blank"><font><font color="#2D61FF">ICCV 2023</font></font></a>.</li>
		
		<li><font style="font-weight:bold; line-height: 120%">2023-05:</font> Appointed as an Area Chair for <a href="https://wacv2024.thecvf.com/" target="_blank"><font><font color="#2D61FF">WACV 2024</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-05:</font> Release MaGIC (<a href="https://arxiv.org/abs/2305.11818" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/yeates/MaGIC" target="_blank"><font><font color="#2D61FF">code</font></font></a>) for multi-modality guided image completion and generation.</li>

		
		<li><font style="font-weight:bold; line-height: 120%">2023-03:</font> Check out PlanarTrack (<a href="https://arxiv.org/abs/2303.07625" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://hengfan2010.github.io/projects/PlanarTrack/" target="_blank"><font><font color="#2D61FF">project</font></font></a>) for large-scale challenging planar object tracking.</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-02:</font> A paper on visual object tracking is accepted to The Innovation.</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-01:</font> Welcome PhD student Shaohua Dong to join our team.</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-01:</font> Check out our new preprint (<a href="https://arxiv.org/abs/2301.00371" target="_blank"><font><font color="#2D61FF">arXiv</font></font></a>) on domain adaptive object detection.</li>
		<li><font style="font-weight:bold; line-height: 120%">2022-11:</font> Our AnimalTrack (<a href="https://arxiv.org/abs/2205.00158" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://hengfan2010.github.io/projects/AnimalTrack/" target="_blank"><font><font color="#2D61FF">project</font></font></a>) is accepted to IJCV.</li>
		
		

		<li><font style="font-weight:bold; line-height: 120%">2022-11:</font> Recognized as the <b>World's Top 2% Scientists</b> (year 2021) by Stanford University. Check <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/3" target="_blank"><font><font color="#2D61FF">here</font></font></a> or <a href="https://research.unt.edu/news/unt-faculty-named-among-world%E2%80%99s-most-cited-researchers" target="_blank"><font><font color="#2D61FF">UNT News</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2022-11:</font> Congrats to Xiaoqiong for her first paper accepted to <a href="https://www.mmm2023.no/" target="_blank"><font><font color="#2D61FF">MMM 2023</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2022-09:</font> Two papers, SwinTrack (<a href="./publication/SwinTrack-NeurIPS-2022.pdf" target="_blank"><font color="#2D61FF">pdf</font></a>, <a href="https://github.com/litinglin/swintrack" target="_blank"><font color="#2D61FF">code</font></a>) and VLT (<a href="./publication/VLT-NeurIPS-2022.pdf" target="_blank"><font color="#2D61FF">pdf</font></a>, <a href="https://github.com/JudasDie/SOTS" target="_blank"><font color="#2D61FF">code</font></a>), on visual tracking are accepted to <a href="https://nips.cc/" target="_blank"><font><font color="#2D61FF">NeurIPS 2022</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2022-09:</font> A paper on domain distribution alignment (<a href="./publication/MedIA-2022.pdf" target="_blank"><font color="#2D61FF">pdf</font></a>) is accepted to MedIA.</li>
		<li><font style="font-weight:bold; line-height: 120%">2022-07:</font> Work on image inpainting (<a href="./publication/InvertFill-ECCV-2022.pdf" target="_blank"><font color="#2D61FF">pdf</font></a>) is accepted to <a href="https://eccv2022.ecva.net/" target="_blank"><font><font color="#2D61FF">ECCV 2022</font></font></a>.</li>
		
		<li><font style="font-weight:bold; line-height: 120%">2022-05:</font> Serve as an Area Chair for <a href="https://wacv2023.thecvf.com/" target="_blank"><font><font color="#2D61FF">WACV 2023</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2022-05:</font> Release AnimalTrack (<a href="https://arxiv.org/abs/2205.00158" target="_blank"><font><font color="#2D61FF">arXiv</font></font></a>) for multi-animal tracking. Stay tuned for updates.</li>
		<li><font style="font-weight:bold; line-height: 120%">2022-04:</font> Our work <a href="https://arxiv.org/abs/2201.02526" target="_blank"><font><font color="#2D61FF">InBN</font></font></a> (<a href="https://arxiv.org/abs/2201.02526" target="_blank"><font color="#2D61FF">pdf</font></a>, <a href="https://github.com/JudasDie/SOTS" target="_blank"><font color="#2D61FF">code</font></a>) on tracking is accepted to <a href="https://www.ijcai.org/" target="_blank"><font><font color="#2D61FF"> IJCAI 2022</font></font></a> as <b>Long Oral</b>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2022-01:</font> Check <a href="https://arxiv.org/abs/2201.02526" target="_blank"><font><font color="#2D61FF">InBN</font></font></a> on arXiv, a simple, general and effective backbone for improving tracking.</li>
		<li><font style="font-weight:bold; line-height: 120%">2022-01:</font> Welcome PhD student Xiaoqiong Liu to join our team.</li>
		<li><font style="font-weight:bold; line-height: 120%">2021-11:</font> A paper on image generation accepted to Pattern Recognition.</li>
		<li><font style="font-weight:bold; line-height: 120%">2021-10:</font> A paper on UAV tracking and detection accepted to PAMI.</li>
		<li><font style="font-weight:bold; line-height: 120%">2021-08:</font> TOTB has been released, go check it out <a href="https://hengfan2010.github.io/projects/TOTB/" target="_blank"><font><font color="#2D61FF"> here</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2021-07:</font> A paper on transparent object tracking accepted to <a href="http://iccv2021.thecvf.com/home" target="_blank"><font><font color="#2D61FF"> ICCV 2021</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2021-07:</font> I will serve as an Area Chair for <a href="http://wacv2022.thecvf.com/home"><font><font color="#2D61FF"> WACV 2022</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2021-07:</font> We have one paper on visual object tracking accepted to <a href="https://www.iros2021.org/"><font><font color="#2D61FF"> IROS 2021</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2021-05:</font> I passed my PhD dissertation. So much gratitude for everyone I met along this journey.</li> 
		-->
    </ul>
	
	
	<HR>
	
	<font size="4" color="black"><b>Quick Links</b></font> <font size="3"> 
		<br>
		&nbsp; &nbsp; &nbsp;
		<a href="https://my.unt.edu/psp/ps/?cmd=login" target="_blank"><font color="#2D61FF" style="line-height: 150%;">myUNT</font></a> &nbsp;   
		<a href="https://it.unt.edu/eagleconnect" target="_blank"><font color="#2D61FF" style="line-height: 150%;">EagleConnect</font></a> &nbsp;   
		<a href="https://computerscience.engineering.unt.edu/" target="_blank"><font color="#2D61FF">UNT CSE</font></a>   &nbsp; 
		<a href="https://www.unt.edu/find-people-departments" target="_blank"><font color="#2D61FF">UNT Directory</font></a>   &nbsp; 
		<a href="https://transportation.unt.edu/sites/default/files/unt_campus_parking.pdf" target="_blank"><font color="#2D61FF">UNT Map</font></a>   &nbsp; 
		<a href="https://www.google.com/" target="_blank"><font color="#2D61FF">Google</font></a>     &nbsp; 
		<a href="https://scholar.google.com/" target="_blank"><font color="#2D61FF">gScholar</font></a>   &nbsp; 
		<a href="http://maps.google.com/" target="_blank"><font color="#2D61FF">gMap</font></a>   &nbsp; 
		<a href="https://www.thecvf.com/" target="_blank"><font color="#2D61FF">CVF</font></a>   &nbsp; 
		<a href="https://ieeexplore.ieee.org/Xplore/home.jsp" target="_blank"><font color="#2D61FF">IEEE</font></a> &nbsp; 
		<a href="https://csrankings.org/#/index?all&us" target="_blank"><font color="#2D61FF">CSRankings</font></a>
	
	<HR>
	
	
</div>

</body></html>
